{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned htmlurl304_output.txt\n",
      "Cleaned htmlurl304l_output.txt\n",
      "Cleaned htmlurl310_output.txt\n",
      "Cleaned htmlurl316_output.txt\n",
      "Cleaned htmlurl316l_output.txt\n",
      "Cleaned htmlurl410_output.txt\n",
      "Cleaned htmlurl430_output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def clean_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Replace multiple newlines with a single newline\n",
    "    # This regex replaces occurrences of two or more newlines with just one newline\n",
    "    cleaned_content = content.strip()  # Removes leading and trailing whitespace and newlines\n",
    "    cleaned_content = \"\\n\".join(line.strip() for line in cleaned_content.splitlines() if line.strip())\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_content)\n",
    "\n",
    "# List of files to clean\n",
    "files_to_clean = [\n",
    "    \"htmlurl304_output.txt\",\n",
    "    \"htmlurl304l_output.txt\",\n",
    "    \"htmlurl310_output.txt\",\n",
    "    \"htmlurl316_output.txt\",\n",
    "    \"htmlurl316l_output.txt\",\n",
    "    \"htmlurl410_output.txt\",\n",
    "    \"htmlurl430_output.txt\"\n",
    "]\n",
    "\n",
    "# Iterate through each file and clean it\n",
    "for filename in files_to_clean:\n",
    "    file_path = os.path.join(os.getcwd(), filename)  # Get full path if needed\n",
    "    clean_text_file(file_path)\n",
    "    print(f\"Cleaned {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\shukl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.4.28-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     -------------------------------------- 41.9/41.9 kB 688.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\shukl\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shukl\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 196.4 kB/s eta 0:00:00\n",
      "Downloading regex-2024.4.28-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   -------------------------------------- 269.0/269.0 kB 113.4 kB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   -------------------------------------- 301.8/301.8 kB 194.4 kB/s eta 0:00:00\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.8.1 regex-2024.4.28\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shukl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shukl\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and updated htmlurl304_output.txt\n",
      "Cleaned and updated htmlurl304l_output.txt\n",
      "Cleaned and updated htmlurl310_output.txt\n",
      "Cleaned and updated htmlurl316_output.txt\n",
      "Cleaned and updated htmlurl316l_output.txt\n",
      "Cleaned and updated htmlurl410_output.txt\n",
      "Cleaned and updated htmlurl430_output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer and load stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove stopwords and lowercase the text\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "    # Lemmatize text\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "# List of files to clean\n",
    "files_to_clean = [\n",
    "    \"htmlurl304_output.txt\",\n",
    "    \"htmlurl304l_output.txt\",\n",
    "    \"htmlurl310_output.txt\",\n",
    "    \"htmlurl316_output.txt\",\n",
    "    \"htmlurl316l_output.txt\",\n",
    "    \"htmlurl410_output.txt\",\n",
    "    \"htmlurl430_output.txt\"\n",
    "]\n",
    "\n",
    "# Iterate through each file and clean it\n",
    "for filename in files_to_clean:\n",
    "    file_path = os.path.join(os.getcwd(), filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Clean the content\n",
    "    cleaned_content = clean_text(content)\n",
    "    \n",
    "    # Write the cleaned content back to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_content)\n",
    "    \n",
    "    print(f\"Cleaned and updated {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
